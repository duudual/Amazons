问题理解：quickscore应该只是一个参考得分，不是每个选择优胜率的真实得分。我希望计算一个MC得分，表示通过蒙特卡洛模拟计算出的各个选择的得分，这个得分应该由quickscore计算而来。算法如下：应该是初始时各个选择的MC得分都是一个初始值，因此模拟下一步也是随机选择的，随机迭代到最大深度或者终局时停止，根据此时的qucikscore反向更新过程中每一步的MCscore。然后后来，在进行选择时，就是MC得分更大的，有更大的概率进行选择，这样才能比较真实的模拟出一个理性的智慧的对手的选择。

本质：为什么使用蒙特卡罗模拟？quickscore只是一个反应当前棋局状态的得分，而不是一个反应决策优劣的得分，这就是上面我想要的mcScore. 
在simulate过程中都使用mcScore来依概率决策，这就是在假设 对手也是一个理性的智慧个体下，如果是随机选择，就是在假设 对手是随机下棋的假设下。

**Problem：决策选择过多，导致没法在simulate中都建立树来使用mcScore。使用一个神经网络来实现对该场景下mcScore的构造**

## 版本一：
限制maxdepth的大小.可以理解为maxdepth为20时是走一步看20步，给其限制一个很小的数比如5.
## 版本二：
我的思路是：训练一个价值网络来获取该局面的各个决策的得分。方法是：
 1.数据生成： 我会使用蒙特卡洛模拟，每个决策具有mcScore，初始都是1，在模拟过程中，对弈双方都是根据mcScore，按概率随机选择一个决策，执行到max Depth后，获得该局面的quickscore(quickscore由bfs计算该颜色起点到空余位置的最短步，然后比较两个颜色，累积得到各个颜色的quickscore值),利用该quickscore反向传播更新各个节点的mcScore。从而，我从一个初始局面，得到了初始与中间多个局面，以及各个局面下各个决策的mcScore。这就可以作为训练数据了 
 2.使用合适的模型架构构造价值网络，然后训练 
 3.在使用时，就是根据神经网络输出各个决策的score，然后选择最高的score